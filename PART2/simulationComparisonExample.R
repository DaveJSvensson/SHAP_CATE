########################################################
## Code for Section 4.3 and 4.4.
## This script illustrates one single iteration, whereas 
## the results in the manuscript were generated by
## parallel computing.
## (The full run takes long time).
########################################################
# All packages and versions are stored in the renv file. 
# The environment can be loaded with the command renv::restore(), this will install all the packages with the right version

source("functions.R") ## Support functions

# Simulation settings to vary 
model="S2" # Simulation model used here for illustration (Both S2 and S3 was used in paper)
# GET THE SIM MODEL 
fun_simulate_data <-get_sim_model(model)


NN.RCT <- 1000 # Number of patients in the data set

### FIXED SETTING FOR ALL SIMS
NN.RCT.TESTDATA <- 10000 ## Sample size for test data (Fixed regardless of NN.RCT)
N.TREES <- 100  ## 10000 was used in paper, for both maxtrees for meta.learn, and for SHAPs when using cvboost
EARLY.STOP = 50 # early stop for xgboost
PRINT.EVERY.N <- 500
VERB <- FALSE
DEBG <- FALSE 
N.CV.FOLDS = 3 ## WAS 10 IN SHAP PAPER
## LOOP OVER THESE PROGNOSTIC MAGNITUDES:
BETAS <- c(0, 0.5, 1, 1.5, 2) ### These were considered in section 4.4


INIT.TIME <- date()

#for (BETA in BETAS){ ## we only show one of the BETAS in this script.

seed=1 ## careful with this, if parallel runs, then each iteration needs its unique value.

BETA <- 1  ## This corresponds to section 4.3
  ## PREPARE DATA
  d <- fun_simulate_data(N=NN.RCT, BETA = BETA, seed = seed)
  d0 <- fun_simulate_data(N=NN.RCT.TESTDATA, BETA = BETA, seed = seed) ## testdata
  ## #######################
  d$y <- d$y.obs; d$y.obs <- NULL
  d0$y <- d0$y.obs; d0$y.obs <- NULL
  
  d <- fun.Categ2DummyCoding(data=d, x_cat_var = "x2")
  d0 <- fun.Categ2DummyCoding(data=d0, x_cat_var = "x2")
  
  d$x2 <- NULL 
  d0$x2 <- NULL 
  
  names(d)[names(d)=="ite"] <- "true.ite"
  names(d0)[names(d0)=="ite"] <- "true.ite"
  
  d.train.copy <- d ## using this sometimes, see below
  d.test.copy <- d0 
  
  ## Training and Test baseline biomarker data
  (BIOMARKERS <- names(d)[grep(names(d), pattern="x", fixed=TRUE)])
  X <- as.matrix(d[,BIOMARKERS])
  X0 <- as.matrix(d0[,BIOMARKERS])
  
  ###################################### CATE BELOW; AND LATER SHAP ######################
  # Set propensity scores depending on if it is RWD or not
  if (model == "S2"){
    # Propensity score for model
    pA <- mean(d.train.copy$trt)
    pA2 <- mean(d.test.copy$trt)
    pre.prop <- rep(pA, nrow(d)) ## vector, needed sometimes
    pre.prop0 <- rep(pA2, nrow(d0)) ## vector, needed sometimes 
    
    print(mean(pre.prop))
    
  } else if (model == "S3"){
    pre.prop <- NULL
    pre.prop0 <- NULL
  }else {
    stop("Model needs to be either S2 or S3")
  }
  
  ### ----------------------------------------------------------------------------
  ## The following is just a trick, required because direct and indirect results in different naming conventions:
  ## try it in the console to see why. 
  ####### i.e., cvboost is using  rlearner:::sanitize_input(X, d$trt, d$y) which renames x
  ###### so now we can see that e.g., "covariate_2" is actually "x3" (all this due to x2a, x2b).
  ## -----------------------------------------------------------------------------------
  
  input = rlearner:::sanitize_input(X, d$trt, d$y) ## don't need this object per se
  xx = input$x ## but this is needed: 
  dictionary <- data.frame(colnames(xx), colnames(X))

  ### Fit meta learners
  ### ---------------------------
  cf <- causal_forest(X =X, Y = d$y,
                      num.tree=N.TREES, 
                      W=d$trt,
                      W.hat = pre.prop, 
                      tune.parameters = "all"
  )
  ### ---------------------------
  sboost_fit = sboost3(x=X,
                       w=d$trt,
                       y=d$y,
                       k_folds = N.CV.FOLDS,
                       ntrees_max = N.TREES, 
                       print_every_n = PRINT.EVERY.N,
                       verbose=VERB, 
                       early_stopping_rounds=EARLY.STOP, 
                       DEBUG=DEBG); 
  
  ### ---------------------------
  tboost_fit <- tboost3(x=X, 
                        w=d$trt, 
                        y=d$y, 
                        k_folds_mu1=N.CV.FOLDS, 
                        k_folds_mu0=N.CV.FOLDS,
                        ntrees_max = N.TREES, 
                        print_every_n = PRINT.EVERY.N,
                        verbose=VERB, 
                        early_stopping_rounds=EARLY.STOP, 
                        DEBUG=DEBG);
  
  ### ---------------------------
  xboost_fit = xboost3(x=X, 
                       w=d$trt, 
                       y=d$y, 
                       p_hat=pre.prop, ## pre needed (RCT)!
                       k_folds_mu0 = N.CV.FOLDS, 
                       k_folds_mu1=N.CV.FOLDS,
                       ntrees_max = N.TREES, 
                       print_every_n = PRINT.EVERY.N,
                       verbose=VERB, 
                       early_stopping_rounds=EARLY.STOP, 
                       DEBUG=DEBG); 
  
  ### ------------------------
  rboost_fit <- rboost3_shaps(x=X, 
                              w=d$trt, 
                              y=d$y, 
                              p_hat=pre.prop,  ## pre needed (RCT)!
                              k_folds = N.CV.FOLDS,
                              ntrees_max = N.TREES, 
                              print_every_n = PRINT.EVERY.N,
                              verbose=VERB, 
                              early_stopping_rounds=EARLY.STOP, 
                              DEBUG=DEBG);
  ### ---------------------------
  drboost_fit.shaps <- DRboost3_shaps(x=X, 
                                      w=d$trt, 
                                      y=d$y, 
                                      p_hat=pre.prop,  ## pre needed (RCT)!
                                      k_folds = N.CV.FOLDS,
                                      ntrees_max = N.TREES, 
                                      print_every_n = PRINT.EVERY.N,
                                      verbose=VERB, 
                                      early_stopping_rounds=EARLY.STOP,
                                      DEBUG=DEBG);
  
  ## /////////// Predict Training data, used for indirect SHAPs later ///////////////////
  
  aa <- predict(cf); 
  d.train.copy$CATE.CF  <- aa$predictions
  d.train.copy$CATE.Slearn = predict(sboost_fit, newx = X)
  d.train.copy$CATE.Tlearn = predict(tboost_fit, newx = X) 
  d.train.copy$CATE.Xlearn = predict(xboost_fit, newx = X, new_p_hat =pre.prop) 
  d.train.copy$CATE.Rlearn = rboost_fit$cate.est 
  d.train.copy$CATE.DRlearn = drboost_fit.shaps$cate.est  
  
  ############# SHAP ##########################################################################################
  
  datFor2.C <- data.frame(cate=d.train.copy$CATE.CF, X) 
  datFor2.T <- data.frame(cate=d.train.copy$CATE.Tlearn, X) 
  datFor2.S <- data.frame(cate=d.train.copy$CATE.Slearn, X) 
  datFor2.X <- data.frame(cate=d.train.copy$CATE.Xlearn, X) 
  datFor2.R <- data.frame(cate=d.train.copy$CATE.Rlearn, X)  
  datFor2.DR <- data.frame(cate=d.train.copy$CATE.DRlearn, X)  
  
  ###_________________________
  ind.shap.model.C = cvboost3(x = X, 
                              y = datFor2.C$cate, 
                              objective = "reg:squarederror", 
                              weights = NULL, # no weights here! 
                              k_folds = N.CV.FOLDS, 
                              ntrees_max = N.TREES, 
                              print_every_n = PRINT.EVERY.N, 
                              early_stopping_rounds = EARLY.STOP, 
                              verbose = VERB)
  ind.shaps.obj.C <- shap.values(xgb_model = ind.shap.model.C$xgb_fit, X_train = X) 
  sh.C <- ind.shaps.obj.C$mean_shap_score
  
  ###_________________________
  ind.shap.model.S = cvboost3(x = X, 
                              y = datFor2.S$cate, 
                              objective = "reg:squarederror", 
                              weights = NULL, # no weights here! 
                              k_folds = N.CV.FOLDS, 
                              ntrees_max = N.TREES, 
                              print_every_n = PRINT.EVERY.N, 
                              early_stopping_rounds = EARLY.STOP, 
                              verbose = VERB)
  ind.shaps.obj.S <- shap.values(xgb_model = ind.shap.model.S$xgb_fit, X_train = X) 
  sh.S <- ind.shaps.obj.S$mean_shap_score
  
  ###_________________________
  ind.shap.model.T = cvboost3(x = X, 
                              y = datFor2.T$cate, 
                              objective = "reg:squarederror", 
                              weights = NULL, # no weights here! 
                              k_folds = N.CV.FOLDS, 
                              ntrees_max = N.TREES, 
                              print_every_n = PRINT.EVERY.N, 
                              early_stopping_rounds = EARLY.STOP, 
                              verbose = VERB)
  ind.shaps.obj.T <- shap.values(xgb_model = ind.shap.model.T$xgb_fit, X_train = X) 
  sh.T <- ind.shaps.obj.T$mean_shap_score 
  
  ###_________________________
  ind.shap.model.X = cvboost3(x = X, 
                              y = datFor2.X$cate, 
                              objective = "reg:squarederror", 
                              weights = NULL, # no weights here! 
                              k_folds = N.CV.FOLDS, 
                              ntrees_max = N.TREES, 
                              print_every_n = PRINT.EVERY.N, 
                              early_stopping_rounds = EARLY.STOP, 
                              verbose = VERB)
  ind.shaps.obj.X <- shap.values(xgb_model = ind.shap.model.X$xgb_fit, X_train = X) 
  sh.X <- ind.shaps.obj.X$mean_shap_score 
  
  #########################
  sh.R0 <- rboost_fit$dir.shaps.obj$mean_shap_score 
  sh.R1 <- rboost_fit$ind.shaps.obj$mean_shap_score
  names(sh.R0) <- translate_what_covariate_is_what_x(ranking = sh.R0, dictionary = dictionary)
  names(sh.R1) <- translate_what_covariate_is_what_x(ranking = sh.R1, dictionary = dictionary)
  
  ###_________________________
  sh.DR0 <- drboost_fit.shaps$dir.shaps.obj$mean_shap_score ### CHECK!!
  sh.DR1 <- drboost_fit.shaps$ind.shaps.obj$mean_shap_score ### CHECK!!
  
  names(sh.DR0) <- translate_what_covariate_is_what_x(ranking = sh.DR0, dictionary = dictionary)
  names(sh.DR1) <- translate_what_covariate_is_what_x(ranking = sh.DR1, dictionary = dictionary)
  
  ##########################
  shap_values <- list(S = sh.S, T = sh.T, X = sh.X, R0 = sh.R0, R1 = sh.R1, DR0 = sh.DR0, DR1 = sh.DR1, C = sh.C)
  
  # Combine all shap values into a single data frame
  SH <- do.call(rbind, lapply(names(shap_values), function(label) {
    data.frame(shap = shap_values[[label]], x = names(shap_values[[label]]), what = label)
  }))
  
  # Ensure 'what' is a factor
  SH$what <- factor(SH$what)
  
  # Add additional columns
  SH$NN.RCT <- NN.RCT
  SH$N.TREES <- N.TREES
  SH$seed <- seed
  SH$BETA <- BETA
  rownames(SH) <- NULL
  
  ######### ///////////////////////// SHAP PROFILES /////////////////////////////
  #################### for recovering g1(x3) and g2(x4)
  ######### Code needed for deriving local SHAP and plot against key covariates, 
  ######### See Section 4.7

  # Define a helper function to process data for each method
  process_profile <- function(data, shap_obj, method_name, bias_key = "BIAS0") {
    # Extract relevant columns and compute SHAP values
    profile <- data.frame(
      x3 = data[, "x3"],
      x4 = data[, "x4"],
      sh3 = unlist(shap_obj$shap_score[, "x3"]),
      sh4 = unlist(shap_obj$shap_score[, "x4"]),
      cate = data$cate
    )
    # Add method name and bias term
    profile$method <- method_name
    profile$bias.term <- as.numeric(shap_obj[[bias_key]])
    return(profile)
  }
  
  # Define a helper function to process R and DR learners with local SHAPs. NEED TO TRANSLATE COVARIATE NAMES
  process_r_dr_profiles <- function(data, shap_obj, method_name, dictionary, bias_key = "BIAS0") {
    # Translate covariate names
    shap_obj_score <- shap_obj$shap_score
    names(shap_obj_score) <- translate_what_covariate_is_what_x(ranking = shap_obj_score, dictionary = dictionary)
    
    # Extract relevant columns and compute SHAP values
    profile <- data.frame(
      x3 = data[, "x3"],
      x4 = data[, "x4"],
      sh3 = unlist(shap_obj_score[, "x3"]),
      sh4 = unlist(shap_obj_score[, "x4"]),
      cate = data$cate
    )
    # Add method name and bias term
    profile$method <- method_name
    profile$bias.term <- as.numeric(shap_obj[[bias_key]])
    return(profile)
  }
  
  # Process profiles for each method
  Profile.C.x3.x4 <- process_profile(datFor2.C, ind.shaps.obj.C, "C")
  Profile.S.x3.x4 <- process_profile(datFor2.S, ind.shaps.obj.S, "S")
  Profile.T.x3.x4 <- process_profile(datFor2.T, ind.shaps.obj.T, "T")
  Profile.X.x3.x4 <- process_profile(datFor2.X, ind.shaps.obj.X, "X")
  
  # Process R learners with custom handling for local SHAPs
  Profile.R0.x3.x4 <- process_r_dr_profiles(datFor2.R, rboost_fit$dir.shaps.obj, "R0", dictionary)
  Profile.R1.x3.x4 <- process_r_dr_profiles(datFor2.R, rboost_fit$ind.shaps.obj, "R1", dictionary)
  
  # Process DR learners with custom handling for local SHAPs
  Profile.DR0.x3.x4 <- process_r_dr_profiles(datFor2.DR, drboost_fit.shaps$dir.shaps.obj, "DR0", dictionary)
  Profile.DR1.x3.x4 <- process_r_dr_profiles(datFor2.DR, drboost_fit.shaps$ind.shaps.obj, "DR1", dictionary)
  
  # Combine all profiles into a single data frame
  Profile.Shaps <- rbind(
    Profile.C.x3.x4,
    Profile.S.x3.x4,
    Profile.T.x3.x4,
    Profile.X.x3.x4,
    Profile.R0.x3.x4,
    Profile.R1.x3.x4,
    Profile.DR0.x3.x4,
    Profile.DR1.x3.x4
  )
  
  # Add additional metadata
  Profile.Shaps$n <- NN.RCT
  Profile.Shaps$BETA <- BETA
  rownames(Profile.Shaps) <- NULL
  
head(Profile.Shaps)
  

