require(rlearner) 
require(xgboost) 
require(rlearner) # https://github.com/xnie/rlearner
require(grf)
require(SHAPforxgboost) 
require(xtable) ## See Renv file for version

## simulation function, see section 4 in paper.
fun.g1 <- function(x, cut=0.5, a=0.625, b=5) { 
  signal <- a-b*(x-cut)^2
  signal[x<0] <- a-b*(0-cut)^2
  signal[x>1] <- a-b*(1-cut)^2
  signal
}#EndFun.

## simulation function, see section 4 in paper.
fun.g2= function(x) {
  c = 0.625
  d = 20
  ret=ifelse(x <= 1 & x >= 0, c/(1+exp(-d*(x-0.5))),ifelse(x<0,0,c)) 
  return(ret)
}

## S2 = randomized clinical trial setting, 19 baseline x, many prognostic, two predictive.
fun.Model_S2 <- function(N=1000) {
  x1 <- rnorm(N, 0.5, 1)
  x2 <- sample(1:3, size=N, replace=TRUE) 
  x3 <- rnorm(N, 0.5, 1)
  x4 <- rnorm(N, 0.5, 1) 
  x5 <- rnorm(N, 0.5, 1) 
  x6 <- rnorm(N, 0.5, 1) 
  x7 <- rnorm(N, 0.5, 1) 
  x8 <- rnorm(N, 0.5, 1) 
  x9 <- rnorm(N, 0.5, 1) 
  eps0 <- rnorm(N); eps1 <- rnorm(N)
  Prog_1 <- 100 - (1*x1 + 5*x2)
  Prog_2 <- 2*(x5+x6+x7+x8+x9)
  Prog <- Prog_1 + Prog_2
  Pred <- fun.g1(x3) + fun.g2(x4) 
  y0 <- Prog + eps0 # 
  y1 <- Prog + Pred + eps1 # 
  ite <- Pred
  trt <- rbinom(N, size=1, prob=3/4); 
  y.obs <- y1; 
  y.obs[trt==0] <- y0[trt==0] 
  x10 <- rnorm(N, 0, 1)  
  x11 <- rnorm(N, 0, 1)   
  x12 <- rnorm(N, 0, 1)  
  x13 <- rnorm(N, 0, 1)  
  x14 <- rnorm(N, 0, 1)  
  x15 <- rnorm(N, 0, 1)  
  x16 <- rnorm(N, 0, 1)  
  x17 <- rnorm(N, 0, 1)  
  x18 <- rnorm(N, 0, 1)  
  x19 <- rnorm(N, 0, 1)  
  dat <- data.frame(y.obs, y1, y0, ite, trt, x1, x2, x3, x4, x5, x6, x7, x8, x9,
                    x10, x11, x12, x13, x14, x15, x16, x17, x18, x19)
  dat
}#Endfun

## As above, but was used for studying how the nuisance prognostic strength impacted biomarker discovery.
fun.Model_S2.BETA <- function(N=1000, BETA=1, seed) {
  set.seed(seed) 
  x1 <- rnorm(N, 0.5, 1)
  x2 <- sample(1:3, size=N, replace=TRUE)
  x3 <- rnorm(N, 0.5, 1) 
  x4 <- rnorm(N, 0.5, 1) 
  x5 <- rnorm(N, 0.5, 1) 
  x6 <- rnorm(N, 0.5, 1) 
  x7 <- rnorm(N, 0.5, 1) 
  x8 <- rnorm(N, 0.5, 1) 
  x9 <- rnorm(N, 0.5, 1) 
  eps0 <- rnorm(N); eps1 <- rnorm(N)
  Prog_1 <- 100 - (1*x1 + 5*x2)
  Prog_2 <- 2*(x5+x6+x7+x8+x9)
  Prog <- Prog_1 + Prog_2
  Pred <- fun.g1(x3) + fun.g2(x4) 
  y0 <- BETA*Prog + eps0 
  y1 <- BETA*Prog + Pred + eps1  
  ite <- Pred
  trt <- rbinom(N, size=1, prob=3/4);
  y.obs <- y1; 
  y.obs[trt==0] <- y0[trt==0]
  x10 <- rnorm(N, 0, 1)  # 1
  x11 <- rnorm(N, 0, 1)  # 2 
  x12 <- rnorm(N, 0, 1)  # 3
  x13 <- rnorm(N, 0, 1)  # 4
  x14 <- rnorm(N, 0, 1)  # 5
  x15 <- rnorm(N, 0, 1)  # 6
  x16 <- rnorm(N, 0, 1)  # 7
  x17 <- rnorm(N, 0, 1)  # 8
  x18 <- rnorm(N, 0, 1)  # 9
  x19 <- rnorm(N, 0, 1)  # 10
  dat <- data.frame(y.obs, y1, y0, ite, trt, x1, x2, x3, x4, x5, x6, x7, x8, x9,
                    x10, x11, x12, x13, x14, x15, x16, x17, x18, x19)
  dat
}#Endfun

## True relationship used in S3 for assigning active treatment 
fun.True.Prob.Active <- function(aa, bb, zz) {
  (1 + exp(-aa -bb*zz))^{-1} 
}#EndProb


## As S2.BETA above, but gives obervational data. 
fun.Model_S3.BETA <- function(N=10000, plotit=FALSE, BETA=1, seed) {
  set.seed(seed)
  x1 <- rnorm(N, 0.5, 1)
  x2 <- sample(1:3, size=N, replace=TRUE) 
  x3 <- rnorm(N, 0.5, 1) 
  x4 <- rnorm(N, 0.5, 1)
  x5 <- rnorm(N, 0.5, 1) 
  x6 <- rnorm(N, 0.5, 1) 
  x7 <- rnorm(N, 0.5, 1) 
  x8 <- rnorm(N, 0.5, 1) 
  x9 <- rnorm(N, 0.5, 1) 
  eps0 <- rnorm(N); eps1 <- rnorm(N)
  K1 <-  -(x1 + 5*x2) + 2*(x5+x6+x7+x8+x9)
  K2 <- (fun.g1(x3) + fun.g2(x4)) 
  y0 <- 100 + BETA*K1 + eps0 # 
  y1 <- 100 + BETA*K1 + K2 + eps1 # 
  ite <- K2
  ## ---------------------
  prob.A <- fun.True.Prob.Active(aa=-2.4, bb=-0.2, zz=K1);  sum(prob.A)/N # want it to be 1/4
  ##------------------------
  trt <- rbinom(N, size=1, prob=prob.A); # table(trt)
  
  if (plotit) {
    par(mfrow=c(1,2))
    rrxx <- paste("range prob=", paste0(round(range(prob.A),5), collapse="-"))
    plot(K1, prob.A, ylim=c(-0.1, 1.1), main="S3 probability(trt=1)"); 
    rug(K1, col="grey50"); 
    hist(prob.A[trt==0],col=rgb(0.8,0.8,0.8,0.5), xlim=c(0,1), 
         main="Propensities by arm (S3)", 
         xlab="Prob(Trt=1)", sub=rrxx)
    hist(prob.A[trt==1],col=rgb(0.2,0.2,0.2,0.5), add=T)
  }
  y.obs <- y1;
  y.obs[trt==0] <- y0[trt==0] 
  x10 <- rnorm(N, 0, 1)  
  x11 <- rnorm(N, 0, 1)  
  x12 <- rnorm(N, 0, 1)  
  x13 <- rnorm(N, 0, 1)  
  x14 <- rnorm(N, 0, 1)  
  x15 <- rnorm(N, 0, 1) 
  x16 <- rnorm(N, 0, 1)  
  x17 <- rnorm(N, 0, 1)  
  x18 <- rnorm(N, 0, 1)  
  x19 <- rnorm(N, 0, 1)
  dat <- data.frame(y.obs, y1, y0, ite, trt, x1, x2, x3, x4, x5, x6, x7, x8, x9,
                    x10, x11, x12, x13, x14, x15, x16, x17, x18, x19)
  dat
}#Endfun



# Just convinient for checking simulated data
func.Check.Simulation <- function(d) {
  prop.Trt.Ac <- round(mean(d$trt),3)
  S <- d$ite>0;
  Signature.Size <- mean(S)
  Overall.mean.true.ITE <- mean(d$ite)
  Signature.mean.true.ITE <- mean(d$ite[S])
  Signature.mean.true.Y1Y0 <- mean(d$y1[S]-d$y0[S])
  res <- data.frame(prop.Trt.Ac, 
                    Signature.Size,
                    Overall.mean.true.ITE,
                    Signature.mean.true.ITE,
                    Signature.mean.true.Y1Y0             
  )
  res
}

###### Hot encoding
fun.Categ2DummyCoding <- function(data, x_cat_var="x2") { 
  print(" ...... creating (p-1) dummy coding (p=no. levels)")
  ij=which(names(data)==x_cat_var) ## this!
  Levs <- sort(unique(data[,ij]))
  no.levs <- length(Levs) 
  new_x_names <- paste0(x_cat_var, 
                        letters[1:(no.levs-1)])
  n <- nrow(data)
  DummyMat <- matrix(0, nrow=n, ncol=(no.levs-1)) 
  colnames(DummyMat)<- new_x_names    
  head(DummyMat,4)
  for(val in 1:(no.levs-1)){
    DummyMat[,val] <- ifelse(data[,ij]==val, 1,0)
  }
  ZEROS <- apply(DummyMat,1,sum)==0
  DummyMat[ZEROS,] <- 1 
  data2 <- cbind(data, DummyMat)
  print("...checking levels;")
  print(unique(data2[, c(x_cat_var, new_x_names) ]))
  data2
}##EndFunction

#### Slight modification of default tuning from rlearner package; some limitations introduced to cut down on extreme run times.
#### Still tunes over a plausible grid (and robustness was manually checked).  
cvboost3 <- function (x, y, weights = NULL, k_folds = 10, 
                      ETAs =c(0.005, 0.01, 0.015, 0.025, 0.05, 0.08, 0.1, 0.2),
                      TR.DEPTH = 2:4, 
                      objective = c("reg:squarederror","binary:logistic"), 
                      ntrees_max = 10000, 
                      print_every_n = 500, 
                      early_stopping_rounds = 50,  
                      verbose = FALSE,
                      DEBUG=FALSE)  {
  require(rlearner)
  print(paste0("...initiating cvboost3 at", date()))
  objective = match.arg(objective)
  if (objective == "reg:squarederror") { 
    eval = "rmse"
  } else if (objective == "binary:logistic") {
    eval = "logloss"
  }else {
    stop("objective not defined.")
  }#EndIf.
  if (is.null(weights)) {    
    weights = rep(1, length(y))
  }
  dtrain <- xgboost::xgb.DMatrix(data = x, label = y, weight = weights)
  best_param = list(); best_seednumber = 1234; best_loss = Inf
  nthread = 1
  iter=0
  for (eta.i in ETAs) { #---------
    for (tr.d in TR.DEPTH) { # --------
      iter=iter + 1
      param <- list(objective = objective,
                    eval_metric  = eval,
                    subsample = 0.9, 
                    colsample_bytree = 0.9,
                    eta = eta.i,
                    max_depth = tr.d) 
      seed_number = sample.int(1e+05, 1)[[1]]
      set.seed(seed_number)
      metric = paste("test_", eval, "_mean", sep = "")
      xgb_cv_args = list(data = dtrain,
                         param = param,
                         missing = NA, 
                         nfold = k_folds, 
                         prediction = TRUE, 
                         early_stopping_rounds = early_stopping_rounds, 
                         maximize = FALSE, 
                         nrounds = ntrees_max, 
                         print_every_n = print_every_n, 
                         verbose = verbose, 
                         nthread = nthread, 
                         callbacks = list(xgboost::cb.cv.predict(save_models = TRUE))
      )
      xgb_cvfit <- do.call(xgboost::xgb.cv, xgb_cv_args)
      metric = paste("test_", eval, "_mean", sep = "")
      head(xgb_cvfit$evaluation_log,2)
      min_loss = min(data.frame(xgb_cvfit$evaluation_log)[, metric])
      if (min_loss < best_loss) {
        best_loss = min_loss
        best_seednumber = seed_number
        best_param = param
        best_xgb_cvfit = xgb_cvfit
      }
      if(verbose) { 
        print(paste0("---iter=", iter, " eta=", eta.i, " depth=", tr.d, " (MaxTrees =",ntrees_max, ")"))}
    }#EndFor.inner -----------------
  }#EndFor.outer ------------
  set.seed(best_seednumber)
  xgb_train_args = list(data = dtrain, params = best_param, 
                        nthread = nthread, nrounds = best_xgb_cvfit$best_ntreelimit)
  xgb_fit <- do.call(xgboost::xgb.train, xgb_train_args)
  if(DEBUG) {
    print(paste0("...cvboost3(): optimal no.trees=", best_xgb_cvfit$best_ntreelimit))
  }
  ret = list(xgb_fit = xgb_fit, best_xgb_cvfit = best_xgb_cvfit, 
             best_seednumber = best_seednumber, best_param = best_param, 
             best_loss = best_loss, best_ntreelimit = best_xgb_cvfit$best_ntreelimit)
  class(ret) <- "cvboost"
  ret
}## End CVBOOST3 --------


## Meta learners below are slight modifications of the ones in rlearner https://github.com/xnie/rlearner. (Basically improved run-times, and SHAP derivation).

## T-learning with boosting as baselearner (using cvboost3)
tboost3 <- function (x, w, y, k_folds_mu1 = NULL, k_folds_mu0 = NULL, ntrees_max = 1000, 
                     print_every_n = 100, early_stopping_rounds = 10, 
                     verbose = FALSE, DEBUG=FALSE) 
{
  require(rlearner)
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  x_1 = x[which(w == 1), ]
  x_0 = x[which(w == 0), ]
  y_1 = y[which(w == 1)]
  y_0 = y[which(w == 0)]
  nobs_1 = nrow(x_1)
  nobs_0 = nrow(x_0)
  pobs = ncol(x)
  if (is.null(k_folds_mu1)) {
    k_folds_mu1 = floor(max(3, min(10, nobs_1/4)))
  }
  if (is.null(k_folds_mu0)) {
    k_folds_mu0 = floor(max(3, min(10, nobs_0/4)))
  }
  t_1_fit = cvboost3(x_1, y_1, objective = "reg:squarederror", 
                     k_folds = k_folds_mu1, ntrees_max = ntrees_max,  
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
  t_0_fit = cvboost3(x_0, y_0, objective = "reg:squarederror", 
                     k_folds = k_folds_mu0, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
  y_1_pred = predict(t_1_fit, newx = x)
  y_0_pred = predict(t_0_fit, newx = x)
  tau_hat = y_1_pred - y_0_pred
  ret = list(t_1_fit = t_1_fit, t_0_fit = t_0_fit, y_1_pred = y_1_pred, 
             y_0_pred = y_0_pred, tau_hat = tau_hat)
  class(ret) <- "tboost"
  ret
}#EndFunction



## S-learning with boosting as baselearner (using cvboost3)
sboost3 <- function (x, w, y, k_folds = NULL, ntrees_max = 1000, 
                     print_every_n = 100, early_stopping_rounds = 10,  
                     verbose = FALSE, DEBUG=FALSE) 
{
  require(rlearner)
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  nobs = nrow(x)
  pobs = ncol(x)
  if (is.null(k_folds)) {
    k_folds = floor(max(3, min(10, nobs/4)))
  }
  s_fit = cvboost3(cbind(x, (w - 0.5) * x, (w - 0.5)), y, objective = "reg:squarederror", 
                   k_folds = k_folds, ntrees_max = ntrees_max, 
                   print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                   verbose = verbose, DEBUG=DEBUG)
  mu0_hat = predict(s_fit, newx = cbind(x, (0 - 0.5) * x, (0 - 
                                                             0.5)))
  mu1_hat = predict(s_fit, newx = cbind(x, (1 - 0.5) * x, (1 - 
                                                             0.5)))
  tau_hat = mu1_hat - mu0_hat
  ret = list(s_fit = s_fit, mu0_hat = mu0_hat, mu1_hat = mu1_hat, 
             tau_hat = tau_hat)
  class(ret) <- "sboost"
  ret
}#Endfunction

## R-learning with boosting as baselearner (using cvboost3)
rboost3 <- function (x, w, y, k_folds = NULL, p_hat = NULL, m_hat = NULL, 
                     ntrees_max = 1000, print_every_n = 100, 
                     early_stopping_rounds = 10, verbose = FALSE, DEBUG=FALSE) 
{
  require(rlearner)
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  nobs = nrow(x)
  pobs = ncol(x)
  if (is.null(k_folds)) {
    k_folds = floor(max(3, min(10, length(y)/4)))
  }
  if (is.null(m_hat)) {
    y_fit = cvboost3(x, y, objective = "reg:squarederror", 
                     k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    m_hat = predict(y_fit)
  }
  else {
    y_fit = NULL
  }
  if (is.null(p_hat)) {
    w_fit = cvboost3(x, w, objective = "binary:logistic", 
                     k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    p_hat = predict(w_fit)
  }
  else {
    w_fit = NULL
  }
  y_tilde = y - m_hat
  w_tilde = w - p_hat
  pseudo_outcome = y_tilde/w_tilde
  weights = w_tilde^2 
  tau_fit = cvboost3(x, pseudo_outcome, objective = "reg:squarederror", 
                     weights = weights, k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, 
                     early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose)
  ret = list(tau_fit = tau_fit, pseudo_outcome = pseudo_outcome, 
             weights = weights, w_fit = w_fit, y_fit = y_fit, p_hat = p_hat, 
             m_hat = m_hat)
  class(ret) = "rboost"
  ret
}#EndFunction.

## X-learning with boosting as baselearner (using cvboost3)
xboost3 <- function (x, w, y, k_folds_mu1 = NULL, k_folds_mu0 = NULL, k_folds_p = NULL, 
                     mu1_hat = NULL, mu0_hat = NULL, p_hat = NULL, ntrees_max = 1000, 
                     print_every_n = 100, early_stopping_rounds = 10, 
                     verbose = FALSE, DEBUG=FALSE) 
{
  require(rlearner)
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  x_1 = x[which(w == 1), ]
  x_0 = x[which(w == 0), ]
  y_1 = y[which(w == 1)]
  y_0 = y[which(w == 0)]
  nobs_1 = nrow(x_1)
  nobs_0 = nrow(x_0)
  nobs = nrow(x)
  pobs = ncol(x)
  if (is.null(k_folds_mu1)) {
    k_folds_mu1 = floor(max(3, min(10, nobs_1/4)))
  }
  if (is.null(k_folds_mu0)) {
    k_folds_mu0 = floor(max(3, min(10, nobs_0/4)))
  }
  if (is.null(k_folds_p)) {
    k_folds_p = floor(max(3, min(10, nobs/4)))
  }
  if (is.null(mu1_hat)) {
    t_1_fit = cvboost3(x_1, y_1, objective = "reg:squarederror", 
                       k_folds = k_folds_mu1, ntrees_max = ntrees_max, 
                       print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                       verbose = verbose, DEBUG=DEBUG)
    mu1_hat = predict(t_1_fit, newx = x)
  }
  else {
    t_1_fit = NULL
  }
  if (is.null(mu0_hat)) {
    t_0_fit = cvboost3(x_0, y_0, objective = "reg:squarederror", 
                       k_folds = k_folds_mu0, ntrees_max = ntrees_max, 
                       print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                       verbose = verbose, DEBUG=DEBUG)
    mu0_hat = predict(t_0_fit, newx = x)
  }
  else {
    t_0_fit = NULL
  }
  d_1 = y_1 - mu0_hat[w == 1]
  d_0 = mu1_hat[w == 0] - y_0
  x_1_fit = cvboost3(x_1, d_1, objective = "reg:squarederror", 
                     k_folds = k_folds_mu1, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
  x_0_fit = cvboost3(x_0, d_0, objective = "reg:squarederror", 
                     k_folds = k_folds_mu0, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, 
                     verbose = verbose, DEBUG=DEBUG)
  tau_1_pred = predict(x_1_fit, newx = x)
  tau_0_pred = predict(x_0_fit, newx = x)
  if (is.null(p_hat)) {
    w_fit = cvboost3(x, w, objective = "binary:logistic", 
                     k_folds = k_folds_p, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    p_hat = predict(w_fit)
  }
  else {
    w_fit = NULL
  }
  tau_hat = tau_1_pred * (1 - p_hat) + tau_0_pred * p_hat
  ret = list(t_1_fit = t_1_fit, t_0_fit = t_0_fit, x_1_fit = x_1_fit, 
             x_0_fit = x_0_fit, w_fit = w_fit, mu1_hat = mu1_hat, 
             mu0_hat = mu0_hat, tau_1_pred = tau_1_pred, tau_0_pred = tau_0_pred, 
             p_hat = p_hat, tau_hat = tau_hat)
  class(ret) <- "xboost"
  ret
}#EndFunction

##____________
predict.cvboost <- function(object,
                            newx=NULL,
                            ...) {
  if (is.null(newx)) {
    return(object$best_xgb_cvfit$pred)
  }
  else{
    dtest <- xgboost::xgb.DMatrix(data=newx)
    return(predict(object$xgb_fit, newdata=dtest))
  }
}

## Modified to also render SHAPs in two ways: direct SHAP and Strategy 3 (surrogate).
rboost3_shaps <- function (x, w, y, k_folds = NULL, p_hat = NULL, m_hat = NULL, 
                           ntrees_max = 1000, print_every_n = 100, 
                           early_stopping_rounds = 10, verbose = FALSE, DEBUG=FALSE) 
{
  require(rlearner)
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  nobs = nrow(x)
  pobs = ncol(x)
  if (is.null(k_folds)) {
    k_folds = floor(max(3, min(10, length(y)/4)))
  }
  if (is.null(m_hat)) {
    y_fit = cvboost3(x, y, objective = "reg:squarederror", 
                     k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    m_hat = predict(y_fit)
  }
  else {
    y_fit = NULL
  }
  if (is.null(p_hat)) {
    w_fit = cvboost3(x, w, objective = "binary:logistic", 
                     k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    p_hat = predict(w_fit)
  }
  else {
    w_fit = NULL
  }
  y_tilde = y - m_hat
  w_tilde = w - p_hat
  pseudo_outcome = y_tilde/w_tilde
  weights = w_tilde^2 
  tau_fit = cvboost3(x, pseudo_outcome, 
                     objective = "reg:squarederror", 
                     weights = weights, k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, 
                     early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose)
  
  cate.est <- predict(tau_fit, newx = x)
  
  ind.shap.model = cvboost3(x = x, y = cate.est, 
                            objective = "reg:squarederror", 
                            weights = NULL, 
                            k_folds = k_folds, 
                            ntrees_max = ntrees_max, 
                            print_every_n = print_every_n, 
                            early_stopping_rounds = early_stopping_rounds, 
                            verbose = verbose)
  ind.shaps.obj <- shap.values(xgb_model = ind.shap.model$xgb_fit, X_train = x) 
  dir.shaps.obj <- shap.values(xgb_model = tau_fit$xgb_fit, X_train = x) 
  ret = list(tau_fit = tau_fit, pseudo_outcome = pseudo_outcome, 
             weights = weights, w_fit = w_fit, y_fit = y_fit, p_hat = p_hat, 
             m_hat = m_hat, ind.shaps.obj=ind.shaps.obj, dir.shaps.obj=dir.shaps.obj, 
             cate.est=cate.est, ind.shap.model=ind.shap.model)
  class(ret) = "rboost"
  ret
}#EndFunction.


## Modified DR to also render SHAPs in two ways: direct SHAP and Strategy 3 (surrogate).
DRboost3_shaps <- function (x, w, y, k_folds = NULL, p_hat = NULL, m_hat = NULL, 
                            ntrees_max = 1000, print_every_n = 100, 
                            early_stopping_rounds = 10, verbose = FALSE, DEBUG=FALSE) 
{
  input = rlearner:::sanitize_input(x, w, y)
  x = input$x
  w = input$w
  y = input$y
  
  nobs = nrow(x)
  pobs = ncol(x)
  if (is.null(k_folds)) {
    k_folds = floor(max(3, min(10, length(y)/4)))
  }
  
  mu_fit_trt <- cvboost3(cbind(x,w), y, objective = "reg:squarederror", 
                         k_folds = k_folds, ntrees_max = ntrees_max, 
                         print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                         verbose = verbose, DEBUG=DEBUG)
  
  
  if (is.null(p_hat)) {
    w_fit = cvboost3(x, w, objective = "binary:logistic", 
                     k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose, DEBUG=DEBUG)
    p_hat = predict(w_fit)
  } else {
    w_fit = NULL
  }
  
  ## prediction under control
  m0 <- predict(mu_fit_trt, newx = cbind(x,w=0))
  ## prediction under treatment
  m1 <- predict(mu_fit_trt, newx = cbind(x,w=1))
  ## prediction under observed arm
  mtrt <- predict(mu_fit_trt, newx = cbind(x,w))
  ## predictions needed for treatment
  pi <- p_hat
  
  ## Calculate pseudo_outcome
  tmp1 <- (m1 - m0)
  L <- w*(y - m1)/pi 
  R <- (1-w)*(y - m0)/(1-pi)
  tmp2 <- L - R
  pseudo_outcome <- tmp1  + tmp2
  #####
  
  ## Calculate CATE
  tau_fit = cvboost3(x, pseudo_outcome, 
                     objective = "reg:squarederror", 
                     weights = NULL, k_folds = k_folds, ntrees_max = ntrees_max, 
                     print_every_n = print_every_n, 
                     early_stopping_rounds = early_stopping_rounds, 
                     verbose = verbose)
  dir.shaps.obj <- shap.values(xgb_model = tau_fit$xgb_fit, X_train = x) 
  cate.est <- predict(tau_fit, x = x)
  ind.shap.model = cvboost3(x = x, y = cate.est, 
                            objective = "reg:squarederror", 
                            weights = NULL, 
                            k_folds = k_folds, 
                            ntrees_max = ntrees_max, 
                            print_every_n = print_every_n, 
                            early_stopping_rounds = early_stopping_rounds, 
                            verbose = verbose)
  ind.shaps.obj <- shap.values(xgb_model = ind.shap.model$xgb_fit, X_train = x)
  ret = list(tau_fit = tau_fit, pseudo_outcome = pseudo_outcome, 
             weights = weights, w_fit = w_fit, 
             p_hat = p_hat, 
             m_hat = m_hat, 
             ind.shaps.obj=ind.shaps.obj, 
             dir.shaps.obj=dir.shaps.obj, 
             cate.est=cate.est, 
             ind.shap.model=ind.shap.model)
  class(ret) = "DRboost"
  ret
}



predict.DRboost<- function(object,
                           newx = NULL,
                           tau_only = T,
                           ...) {
  if (!is.null(newx)){
    newx = rlearner:::sanitize_x(newx)
  }
  if (tau_only) {
    predict(object$tau_fit, newx = newx)
  } else {
    tau = predict(object$tau_fit, newx = newx)
    e = predict(object$w_fit, newx = newx)
    m = predict(object$y_fit, newx = newx)
    mu1 = m + (1-e) * tau
    mu0 = m - e * tau
    return(list(tau=tau, e=e, m=m, mu1 = mu1, mu0 = mu0))
  }
}


get_sim_model <- function(model){
  
  if (model =="S2"){
    fun_simulate_data <- fun.Model_S2.BETA
  }else if (model =="S3"){
    fun_simulate_data <- fun.Model_S3.BETA
  }
  else{
    stop("Model needs to be either S2 or S3")
  }
}#EndFunction.

## Sometimes required when rlearner package reverts names from "x" to "covariate"
translate_what_covariate_is_what_x <- function(ranking, dictionary) {
  here <- match(
    names(ranking), 
    dictionary[,1]
  )
  dictionary[here,2]  
}#EndFunc.

